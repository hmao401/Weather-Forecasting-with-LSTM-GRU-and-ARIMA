{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hmao401/Weather-Forecasting-with-LSTM-GRU-and-ARIMA/blob/main/GHCN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DETGTflhZoSh"
      },
      "source": [
        "# Miniproject: Modelling the climate or the weather?\n",
        "\n",
        "Is past performance an indicator of future weather?\n",
        "\n",
        "## Global Historial Climatology Network\n",
        "GHCN (Global Historical Climatology Network)-Daily is an integrated database of daily climate summaries from land surface stations across the globe.\n",
        "\n",
        "THe GHCN has many datasets from weather stations across the globe. A [README describing the data form is available here](http://www.hep.ucl.ac.uk/undergrad/0056/other/projects/ghcnd/readme.txt). The [stations.txt](http://www.hep.ucl.ac.uk/undergrad/0056/other/projects/ghcnd/ghcnd-stations.txt) file and [countries.txt](http://www.hep.ucl.ac.uk/undergrad/0056/other/projects/ghcnd/ghcnd-countries.txt) contain information about the stations and countries.\n",
        "\n",
        "\n",
        "### Machine Learning Tasks:\n",
        "1. Can you design a machine learning technique that can predict the climate (defined as the weekly or monthly average) a year in advance?  [Later in the term files containing the 2021 data will be made available\n",
        "2. Can you design a machine learning technique that can predict the weather (temperature, rainfall, snow fall, etc.) any better than assuming that the weather tomorrow will be exactly the same as the weather today\n",
        "\n",
        "### Potential extensions\n",
        "1. Can you train a machine learning technique to predict 10 or 20 years into the future?\n",
        "2. Where will the hotest part of the world be in 20 years time?\n",
        "3. What else can you study with this dataset? Is the sun in Utah a predictor of the rain in Spain?\n",
        "4. How close do weather stations need to be to provide reliable forecasts at other stations?\n",
        "\n",
        "### Caveats\n",
        "This is real data from weather stations around the world. This means that there are 'holes' in the data. You must be able to handle these 'holes' in some error tolerant fashion.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task Overview"
      ],
      "metadata": {
        "id": "fkiGJwl6NItR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this task, we are working with **climate data** from the **Global Historical Climatology Network Data (GHNCD)**, focusing on station-level temperature and precipitation data. Our goal is to explore and analyze the historical weather data of various stations around the world, particularly focusing on the **TMAX (maximum temperature)**, **TMIN (minimum temperature)**, and **PRCP (precipitation)** variables.\n",
        "\n",
        "To achieve this, we:\n",
        "1. **Examine the GHNCD data files** using custom functions to extract relevant climate variables.\n",
        "2. **Create a structured representation** of each weather station, including its coordinates, elevation, and the historical data for TMAX, TMIN, and PRCP.\n",
        "3. **Visualize the climate data** for the selected stations, identifying patterns and trends in temperature and precipitation over time.\n",
        "4. **Perform statistical analysis** (e.g., correlation analysis) to understand how climate variables interact across different stations and how they evolve over time.\n",
        "\n",
        "This analysis will provide insight into climate trends, helping to identify regions with significant temperature or precipitation shifts, which can be valuable for further research or climate modeling tasks.\n"
      ],
      "metadata": {
        "id": "vu2bMHmzNqME"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H00WS9_gVVuR"
      },
      "source": [
        "### Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szVjWddjQrnQ"
      },
      "outputs": [],
      "source": [
        "# Import all necessary packages\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from xgboost import XGBRegressor\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Lambda, Input\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, r2_score, confusion_matrix, accuracy_score\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from geopy.distance import geodesic\n",
        "from scipy.optimize import curve_fit\n",
        "from math import radians, cos, sin, sqrt, atan2\n",
        "import itertools\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTxE0HSCZoSk"
      },
      "outputs": [],
      "source": [
        "# Class representing a weather station with relevant attributes\n",
        "class Station():\n",
        "    def __init__(self, sid, lat, lon, el, state, name, gsn, hcn, wmo, country):\n",
        "        self.sid = sid          # Station ID\n",
        "        self.lat = lat          # Latitude\n",
        "        self.lon = lon          # Longitude\n",
        "        self.el = el            # Elevation\n",
        "        self.state = state      # State or region\n",
        "        self.name = name        # Station name\n",
        "        self.gsn = gsn          # Global Surface Network (GSN) indicator\n",
        "        self.hcn = hcn          # Historical Climatology Network (HCN) indicator\n",
        "        self.wmo = wmo          # WMO station code\n",
        "        self.country = country  # Country code\n",
        "\n",
        "    def __str__(self):\n",
        "        # Return a formatted string of station info\n",
        "        return self.sid + \" is \" + self.name + \", \" + self.country + \" at \" + str(self.lat) + \", \" + str(self.lon) + \", \" + str(self.el)\n",
        "\n",
        "# Class to manage the Global Historical Climatology Network Data (GHNCD)\n",
        "class GHNCD:\n",
        "    def __init__(self):\n",
        "        # Column lengths for station data fields\n",
        "        self.station_col_len = [11, 4, 2, 4]\n",
        "        for _ in range(31):\n",
        "            self.station_col_len.extend([5, 3])\n",
        "\n",
        "    def chunkstring(self, string, lengths):\n",
        "        # Split a string into chunks based on the given lengths\n",
        "        return (string[pos:pos+length].strip()\n",
        "                for idx, length in enumerate(lengths)\n",
        "                for pos in [sum(map(int, lengths[:idx]))])\n",
        "\n",
        "    def processFile(self, fileName):\n",
        "        # Process the .dly file and return data in dictionary format\n",
        "        outDict = {}\n",
        "        with open(fileName, 'r') as fp:\n",
        "            line = fp.readline()\n",
        "            while line:\n",
        "                # Split data into fields based on column lengths\n",
        "                fields = list(self.chunkstring(line, self.station_col_len))\n",
        "                station = fields[0]\n",
        "                year = int(fields[1])\n",
        "                month = int(fields[2])\n",
        "                field = fields[3]\n",
        "                vals = fields[4::2]\n",
        "                flags = fields[5::2]\n",
        "\n",
        "                # Helper function to handle missing data (flag = '')\n",
        "                def checkInt(x, flag):\n",
        "                    if flag == '':\n",
        "                        return -9999\n",
        "                    return int(x)\n",
        "\n",
        "                ivals = [checkInt(x, flag) for x, flag in zip(vals, flags)]\n",
        "                monthDict = dict(year=year, month=month, field=field, vals=ivals, flags=flags)\n",
        "\n",
        "                # Store the data in the output dictionary, keyed by variable field\n",
        "                if field in outDict:\n",
        "                    outDict[field]['monthList'].append(monthDict)\n",
        "                else:\n",
        "                    outDict[field] = dict(monthList=[monthDict])\n",
        "                line = fp.readline()\n",
        "        return outDict\n",
        "\n",
        "    def readCountriesFile(self, fileName=None):\n",
        "        # Read country codes and names from a file\n",
        "        self.countryDict = {}\n",
        "        if fileName is None:\n",
        "            file = urllib.request.urlopen('http://www.hep.ucl.ac.uk/undergrad/0056/other/projects/ghcnd/ghcnd-countries.txt')\n",
        "        else:\n",
        "            file = open(fileName, 'r')\n",
        "\n",
        "        # Read country codes and their corresponding names\n",
        "        for line in file:\n",
        "            c = line[0:2].strip()\n",
        "            d = line[3:].strip()\n",
        "            self.countryDict[c] = d\n",
        "        print(\"Read\", len(self.countryDict), \"countries and codes\")\n",
        "\n",
        "    def readStationsFile(self, fileName=None, justGSN=True):\n",
        "        # Read station data from a file and populate station dictionary\n",
        "        self.stationDict = {}\n",
        "        if fileName is None:\n",
        "            file = urllib.request.urlopen('http://www.hep.ucl.ac.uk/undergrad/0056/other/projects/ghcnd/ghcnd-stations.txt')\n",
        "        else:\n",
        "            file = open(fileName, 'r')\n",
        "\n",
        "        # Read and process station data\n",
        "        for line in file:\n",
        "            sid = line[0:11].strip()  # Station ID\n",
        "            lat = float(line[12:20].strip())  # Latitude\n",
        "            lon = float(line[21:30].strip())  # Longitude\n",
        "            el = float(line[31:37].strip())  # Elevation\n",
        "            state = line[38:40].strip()  # State\n",
        "            name = line[41:71].strip()  # Station name\n",
        "            gsn = line[72:75].strip()  # GSN code\n",
        "            hcn = line[76:79].strip()  # HCN code\n",
        "            wmo = line[80:85].strip()  # WMO code\n",
        "\n",
        "            # Skip stations without GSN if 'justGSN' flag is set\n",
        "            if justGSN and gsn == '':\n",
        "                continue\n",
        "\n",
        "            # Get country name from countryDict\n",
        "            country_code = sid[:2]\n",
        "            country = self.countryDict.get(country_code, \"Unknown\")\n",
        "            self.stationDict[sid] = Station(sid, lat, lon, el, state, name, gsn, hcn, wmo, country)\n",
        "\n",
        "        print(\"Read\", len(self.stationDict), \"stations with justGSN\", justGSN)\n",
        "\n",
        "    def getVar(self, statDict, varName='TMAX'):\n",
        "        # Retrieve variable data (e.g., TMAX, TMIN, PRCP)\n",
        "        cal = 0.1 if varName not in ['SNOW', 'SNWD'] else 1.0\n",
        "        tempList = [\n",
        "            (datetime.date(month['year'], month['month'], ind+1), cal * val)\n",
        "            for month in statDict.get(varName, {}).get('monthList', [])\n",
        "            for ind, val in enumerate(month['vals'])\n",
        "            if val != -9999\n",
        "        ]\n",
        "        return tempList\n",
        "\n",
        "    def getTMAX(self, statDict):\n",
        "        # Wrapper for getVar() to fetch TMAX specifically\n",
        "        return self.getVar(statDict, 'TMAX')\n",
        "\n",
        "    def printStation(self, sid):\n",
        "        # Print station details for a given station ID\n",
        "        print(self.stationDict[sid])\n",
        "\n",
        "    def getStation(self, sid):\n",
        "        # Return the Station object for a given station ID\n",
        "        return self.stationDict[sid]\n",
        "\n",
        "    def getStatKeyNames(self):\n",
        "        # Return a list of station IDs\n",
        "        return list(self.stationDict.keys())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "AZysbdAcBneY",
        "outputId": "1a9f59bf-5337-4c47-9ce6-37e2289963f4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'ghcnd-countries.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d20511d7667f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# --- Dynamically find station files and metadata ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mghn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGHNCD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mghn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadCountriesFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ghcnd-countries.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mghn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadStationsFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ghcnd-stations.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjustGSN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-d52cd2c93804>\u001b[0m in \u001b[0;36mreadCountriesFile\u001b[0;34m(self, fileName)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'http://www.hep.ucl.ac.uk/undergrad/0056/other/projects/ghcnd/ghcnd-countries.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ghcnd-countries.txt'"
          ]
        }
      ],
      "source": [
        "# Find station files\n",
        "ghn = GHNCD()\n",
        "ghn.readCountriesFile(\"ghcnd-countries.txt\")\n",
        "ghn.readStationsFile(\"ghcnd-stations.txt\", justGSN=True)\n",
        "\n",
        "base_dir = \"ghcnd_gsn\"  # Folder where files are unpacked"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9j_Ed9_gZoSl"
      },
      "outputs": [],
      "source": [
        "# Get list of station names\n",
        "statNames=ghn.getStatKeyNames()\n",
        "\n",
        "# Arbitrary number from 0 to 990\n",
        "whichStat=220\n",
        "fileName=statNames[whichStat]+'.dly'\n",
        "urlName='http://www.hep.ucl.ac.uk/undergrad/0056/other/projects/ghcnd/ghcnd_gsn/'+fileName\n",
        "\n",
        "# Copy a network object to a local file\n",
        "urllib.request.urlretrieve(urlName,fileName)\n",
        "statDict=ghn.processFile(fileName)\n",
        "print(ghn.getStation(statNames[whichStat]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GW5tsTbEZoSl"
      },
      "outputs": [],
      "source": [
        "# Retrieve temperature data for TMAX (Max Temp) and TMIN (Min Temp)\n",
        "tmaxArray = ghn.getVar(statDict, 'TMAX')\n",
        "days, tmax = zip(*tmaxArray)\n",
        "\n",
        "tminArray = ghn.getVar(statDict, 'TMIN')\n",
        "days2, tmin = zip(*tminArray)\n",
        "\n",
        "# Print the number of available data points\n",
        "print(len(days))\n",
        "\n",
        "# Create plots for TMAX and TMIN over time\n",
        "fig, ax = plt.subplots(2, 1)\n",
        "ax[0].plot(days, tmax, '.')\n",
        "ax[0].plot(days2, tmin, '.')\n",
        "ax[0].set_xlabel(\"date\")\n",
        "ax[0].set_ylabel(\"Temperature (C)\")\n",
        "ax[0].set_title(ghn.getStation(statNames[whichStat]))\n",
        "\n",
        "# Plot the last 1000 days of data\n",
        "ax[1].plot(days[-1000:], tmax[-1000:], '.')\n",
        "ax[1].plot(days2[-1000:], tmin[-1000:], '.')\n",
        "ax[1].set_xlabel(\"date\")\n",
        "ax[1].set_ylabel(\"Temperature (C)\")\n",
        "\n",
        "# Retrieve and unzip precipitation and snowfall data\n",
        "prcpArray = ghn.getVar(statDict, 'PRCP')\n",
        "snowArray = ghn.getVar(statDict, 'SNOW')\n",
        "days_prcp, prcp = zip(*prcpArray)\n",
        "days_snow, snow = zip(*snowArray)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQE-zfeaZoSm"
      },
      "source": [
        "## Cleaning the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA4NChWz1u0n"
      },
      "source": [
        "### Cleaning/filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFV9ocVkZoSm"
      },
      "source": [
        "Before performing any analysis on the climate data, it is crucial to ensure that the dataset is **clean** and **well-structured**. Raw climate data can contain **missing values**, **outliers**, or **invalid entries**, which can distort the results of the analysis.\n",
        "\n",
        "In this section, we will:\n",
        "1. **Remove or handle missing values** (e.g., entries with -9999, which indicate missing or erroneous data).\n",
        "2. **Filter out irrelevant data**: If certain variables or stations are not required for the analysis, we will discard them to reduce noise and focus on the most relevant data.\n",
        "3. **Ensure proper formatting**: Ensure all data is in the correct format (e.g., dates are in `datetime` format, numerical values are appropriately scaled).\n",
        "\n",
        "By performing these cleaning and filtering steps, we can ensure that the subsequent analysis and modeling will be based on **accurate** and **reliable** data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWvBrBYy53St"
      },
      "outputs": [],
      "source": [
        "# Check length consistency before merging\n",
        "print(len(days), len(tmax), len(tmin))\n",
        "\n",
        "# Create DataFrames for TMAX and TMIN\n",
        "df_tmax = pd.DataFrame({'date': days, 'TMAX': tmax})\n",
        "df_tmin = pd.DataFrame({'date': days2, 'TMIN': tmin})\n",
        "\n",
        "# Convert 'date' column to datetime format\n",
        "df_tmax[\"date\"] = pd.to_datetime(df_tmax[\"date\"], errors=\"coerce\")\n",
        "df_tmin[\"date\"] = pd.to_datetime(df_tmin[\"date\"], errors=\"coerce\")\n",
        "\n",
        "# Merge TMAX and TMIN dataframes on date\n",
        "df_weather = pd.merge(df_tmax, df_tmin, on=\"date\", how=\"outer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXlXELb7Jpfp"
      },
      "outputs": [],
      "source": [
        "# Replace invalid (-9999) values with NaN\n",
        "df_weather.replace(-9999, np.nan, inplace=True)\n",
        "\n",
        "# Convert 'date' column to datetime\n",
        "df_weather['date'] = pd.to_datetime(df_weather['date'], errors=\"coerce\")\n",
        "\n",
        "# Ensure 'date' is set as the index for merging\n",
        "df_weather.set_index('date', inplace=True)\n",
        "\n",
        "# Create DataFrames for PRCP and SNOW\n",
        "df_prcp = pd.DataFrame({'date': pd.to_datetime(days_prcp, errors=\"coerce\"), 'PRCP': prcp}).set_index('date')\n",
        "df_snow = pd.DataFrame({'date': pd.to_datetime(days_snow, errors=\"coerce\"), 'SNOW': snow}).set_index('date')\n",
        "\n",
        "# Merge precipitation data\n",
        "df_weather = df_weather.merge(df_prcp, left_index=True, right_index=True, how='outer')\n",
        "\n",
        "# Merge snowfall data, handling potential column name conflicts\n",
        "df_weather = df_weather.merge(df_snow, left_index=True, right_index=True, how='outer', suffixes=('', '_snow'))\n",
        "\n",
        "# If 'SNOW_snow' column was created during merge, combine it with 'SNOW' and drop it\n",
        "if 'SNOW_snow' in df_weather.columns:\n",
        "    df_weather['SNOW'] = df_weather['SNOW'].combine_first(df_weather['SNOW_snow'])\n",
        "    df_weather.drop(columns=['SNOW_snow'], inplace=True)\n",
        "\n",
        "# Reset index after merging\n",
        "df_weather.reset_index(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAcRA6Ia53Vh"
      },
      "outputs": [],
      "source": [
        "# Replace invalid (-9999) values with NaN\n",
        "df_weather.replace(-9999, np.nan, inplace=True)\n",
        "\n",
        "# Create DataFrames for PRCP and SNOW\n",
        "df_prcp = pd.DataFrame({'date': pd.to_datetime(days_prcp, errors=\"coerce\"), 'PRCP': prcp})\n",
        "df_snow = pd.DataFrame({'date': pd.to_datetime(days_snow, errors=\"coerce\"), 'SNOW': snow})\n",
        "\n",
        "# Merge precipitation data\n",
        "df_weather = df_weather.merge(df_prcp, on='date', how='outer')\n",
        "\n",
        "# Merge snowfall data and restore missing values\n",
        "df_weather = df_weather.merge(df_snow, on='date', how='outer', suffixes=('', '_snow'))\n",
        "df_weather['SNOW'] = df_weather['SNOW'].combine_first(df_weather['SNOW_snow'])\n",
        "df_weather.drop(columns=['SNOW_snow'], inplace=True)  # Cleanup redundant column\n",
        "\n",
        "# Ensure date column is datetime format again after merging\n",
        "df_weather['date'] = pd.to_datetime(df_weather['date'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAY55k1X53YP"
      },
      "outputs": [],
      "source": [
        "# Handle missing snowfall data properly\n",
        "df_weather['SNOW'].fillna(df_snow['SNOW'], inplace=True)  # Use real snowfall data where possible\n",
        "df_weather['SNOW'].fillna(0, inplace=True)  # Set remaining missing SNOW values to 0\n",
        "\n",
        "# Apply forward-fill and backward-fill to other missing values\n",
        "df_weather.fillna(method='ffill', inplace=True)\n",
        "df_weather.fillna(method='bfill', inplace=True)\n",
        "\n",
        "# Print sanity checks\n",
        "print(df_weather.shape)  # Shape of the DataFrame\n",
        "print(df_weather.head(10))  # First 10 rows\n",
        "print(df_weather.tail(10))  # Last 10 rows\n",
        "print(df_weather.isna().sum())  # Count remaining NaN values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBFwFRvd66Vt"
      },
      "outputs": [],
      "source": [
        "# Resolve duplicate PRCP columns after merging\n",
        "if 'PRCP_x' in df_weather.columns and 'PRCP_y' in df_weather.columns:\n",
        "    df_weather['PRCP'] = df_weather[['PRCP_x', 'PRCP_y']].max(axis=1)  # Take the max value if both exist\n",
        "    df_weather.drop(columns=['PRCP_x', 'PRCP_y'], inplace=True)  # Remove redundant columns\n",
        "\n",
        "# Ensure 'PRCP' column is present before applying transformations\n",
        "if 'PRCP' not in df_weather.columns:\n",
        "    raise KeyError(\"PRCP column is missing after merging. Check the data sources.\")\n",
        "\n",
        "# Log-transform precipitation to normalize distribution\n",
        "df_weather[\"PRCP\"] = np.log1p(df_weather[\"PRCP\"])\n",
        "\n",
        "# Extract month and day of the week for seasonality awareness\n",
        "df_weather['Month'] = df_weather['date'].dt.month\n",
        "df_weather['DayOfWeek'] = df_weather['date'].dt.dayofweek\n",
        "\n",
        "# Add lag features safely\n",
        "df_weather['TMAX_lag1'] = df_weather['TMAX'].shift(1)\n",
        "df_weather['TMIN_lag1'] = df_weather['TMIN'].shift(1)\n",
        "df_weather['PRCP_lag1'] = df_weather['PRCP'].shift(1) if 'PRCP' in df_weather.columns else np.nan\n",
        "df_weather['SNOW_lag1'] = df_weather['SNOW'].shift(1) if 'SNOW' in df_weather.columns else np.nan\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fJYCINc53pC"
      },
      "outputs": [],
      "source": [
        "# Set date as the index for resampling\n",
        "df_weather.set_index(\"date\", inplace=True)\n",
        "\n",
        "# Resample to weekly and monthly averages\n",
        "df_weekly = df_weather.resample(\"W\").mean()\n",
        "df_monthly = df_weather.resample(\"M\").mean()\n",
        "\n",
        "# Fill any remaining missing values\n",
        "df_monthly.ffill(inplace=True)\n",
        "df_monthly.bfill(inplace=True)\n",
        "\n",
        "# Double-check for missing values after resampling\n",
        "print(\"Missing values after resampling:\")\n",
        "print(df_monthly.isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMW4tfolZoSm"
      },
      "outputs": [],
      "source": [
        "# Sanity check\n",
        "print(df_weather.shape)  # (rows, columns)\n",
        "print(df_weather.head(10))  # Show the first 10 rows\n",
        "print(df_weather.tail(10))  # Show the last 10 rows\n",
        "print(df_weather.isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIo6MGASJypZ"
      },
      "source": [
        "### Initial preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnYJS_-uk6A9"
      },
      "outputs": [],
      "source": [
        "# Plots the monthly trends of average temperature (tmin and tmax)\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(df_monthly.index, df_monthly[\"TMAX\"], label=\"TMAX (Monthly Avg)\", color=\"green\")\n",
        "plt.plot(df_monthly.index, df_monthly[\"TMIN\"], label=\"TMIN (Monthly Avg)\", color=\"red\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Temperature (Â°C)\")\n",
        "plt.title(\"Fixed Monthly Average Temperature Trends\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHwIjKuvtf0b"
      },
      "source": [
        "Polynomial and spline interpolation methods were tried in order to fill missing values, but they either introduced instability, unrealistic oscillations, or failed due to date index constraints. As a result, forward-fill (ffill) and backward-fill (bfill) were chosen as the most reliable approach.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxOQkO2tk6Dw"
      },
      "outputs": [],
      "source": [
        "# Seasonal decomposition\n",
        "decomposition = seasonal_decompose(df_monthly[\"TMAX\"], model=\"additive\", period=12)\n",
        "decomposition.plot()\n",
        "plt.show()\n",
        "\n",
        "# Summary of statistics\n",
        "print(\"\\nSummary Statistics:\")\n",
        "print(df_weather.describe())\n",
        "\n",
        "# Calculates rolling average trends\n",
        "df_weather['TMAX_rolling'] = df_weather[\"TMAX\"].rolling(window=365, min_periods=1).mean()\n",
        "df_weather['TMIN_rolling'] = df_weather[\"TMIN\"].rolling(window=365, min_periods=1).mean()\n",
        "\n",
        "# Plots the rolling averages\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df_weather.index, df_weather['TMAX_rolling'], label='TMAX Rolling Avg', color='red')\n",
        "plt.plot(df_weather.index, df_weather['TMIN_rolling'], label='TMIN Rolling Avg', color='blue')\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Temperature (Â°C)\")\n",
        "plt.title(\"Temperature Trends (365-Day Rolling Avg)\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8L46Ufiyu51F"
      },
      "source": [
        "The summary statistics provide insight into the dataset's temperature distribution:\n",
        "- `count`: Number of data points available for TMAX and TMIN.\n",
        "- `mean`: The average temperature over the dataset.\n",
        "- `std`: Standard deviation, indicating variability in temperature.\n",
        "- `min` and `max`: The lowest and highest recorded temperatures.\n",
        "- `25%`, `50% (median)`, `75%`: Quartiles showing the spread of temperature values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbMMWg-uZoSn"
      },
      "source": [
        "The `TMAX` (Maximum Temperature) and `TMIN` (Minimum Temperature) monthly averages demonstrate seasonal variations, with clear periodic fluctuations over time. The rolling average trend indicates long-term temperature shifts, highlighting periods of warming and cooling.\n",
        "Seasonal decomposition shows distinct cyclical patterns in TMAX, confirming\n",
        "the influence of yearly weather cycles.\n",
        "The data also highlights extreme temperature events, which can be valuable for\n",
        "identifying climate anomalies or trends in temperature variability over the years."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBC6JurTz46x"
      },
      "outputs": [],
      "source": [
        "# Plots all precipitation data\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.plot(df_weather.index, np.expm1(df_weather['PRCP']), color='navy', alpha=0.6)\n",
        "plt.title(\"Daily Precipitation Over Time\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Precipitation (mm)\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80OH4_V3aAg5"
      },
      "outputs": [],
      "source": [
        "# Group by year and sum snowfall\n",
        "df_weather['Year'] = df_weather.index.year\n",
        "annual_snowfall = df_weather.groupby('Year')['SNOW'].sum()\n",
        "\n",
        "# Display annual snowfall statistics\n",
        "print(annual_snowfall.describe())\n",
        "print(annual_snowfall)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(annual_snowfall.index, annual_snowfall.values, marker='o', linestyle='-', color='b')\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Total Snowfall (mm)\")\n",
        "plt.title(\"Annual Snowfall Trend\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcFuBZ--ahyL"
      },
      "outputs": [],
      "source": [
        "# Convert to binary snowfall occurrence (1 if snow > 0 on a given day)\n",
        "df_weather['Snow_Occurred'] = (df_weather['SNOW'] > 0).astype(int)\n",
        "\n",
        "# Count the number of snow days per year\n",
        "annual_snow_days = df_weather.groupby('Year')['Snow_Occurred'].sum()\n",
        "\n",
        "# Display annual snow day statistics\n",
        "print(annual_snow_days.describe())\n",
        "print(annual_snow_days)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-JQDZ7UbCwZ"
      },
      "source": [
        "No significant snowfall activity before 2008, in fact after 2008 - there is 0 snowfall occurences. This means snowfall is not a useful predictor, beyond 2008 at least. Model will not be able to learn meaningfull patterns so instead I will focus on temperature and precipiation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MCW2zoYJoer"
      },
      "source": [
        "### Checking for outliers?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUK8UCiEJtwR"
      },
      "outputs": [],
      "source": [
        "# Create a boxplot in attempts to visualise outliers\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.boxplot(data=df_weather[['TMAX', 'TMIN']])\n",
        "plt.title(\"Temperature Outliers\")\n",
        "plt.show()\n",
        "\n",
        "# Calculate the IQR for TMAX and TMIN\n",
        "Q1 = df_weather[['TMAX', 'TMIN']].quantile(0.25)\n",
        "Q3 = df_weather[['TMAX', 'TMIN']].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Define upper and lower bounds for outlier detection\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Identify any possible outliers\n",
        "outliers_TMAX = (df_weather[\"TMAX\"] < lower_bound[\"TMAX\"]) | (df_weather[\"TMAX\"] > upper_bound[\"TMAX\"])\n",
        "outliers_TMIN = (df_weather[\"TMIN\"] < lower_bound[\"TMIN\"]) | (df_weather[\"TMIN\"] > upper_bound[\"TMIN\"])\n",
        "\n",
        "# Output the number of outliers\n",
        "print(\"Number of TMAX outliers:\", outliers_TMAX.sum())\n",
        "print(\"Number of TMIN outliers:\", outliers_TMIN.sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xE3sFf3NQD_"
      },
      "source": [
        "The whiskers or lines extending beyond the box represent the range within 1.5xIQR of the quartiles, and since there are no points/dots outside this range - there seem to be no outliers. TMAX and TMIN appear to behave normally and have expected seasonal variation without any extreme values, well distributed data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVsnAEGhLmmS"
      },
      "outputs": [],
      "source": [
        "# Plotting histograms for TMAX and TMIN\n",
        "plt.figure(figsize=(11, 5))\n",
        "sns.histplot(df_weather[\"TMAX\"], color=\"blue\", kde=True)\n",
        "plt.title(\"TMAX Distribution\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(11, 5))\n",
        "sns.histplot(df_weather[\"TMIN\"], color=\"orange\", bins=30, kde=True)\n",
        "plt.xlabel(\"TMIN\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"TMIN Distribution\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SIBC9OxOPCo"
      },
      "source": [
        "Both distributions are bimodal with TMIN having a lower temperature range than TMAX as expected since it represents the coldest temperature of the day. This suggests nights tend to be much colder - which is a typical diurnal cycle behaviour."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OMdQgVYO7pY"
      },
      "outputs": [],
      "source": [
        "# Plots a scatterplot of a comparison of TMAX and TMIN\n",
        "plt.figure(figsize=(9, 6))\n",
        "sns.scatterplot(x=df_weather[\"TMIN\"], y=df_weather[\"TMAX\"], alpha=0.5)\n",
        "plt.xlabel(\"TMIN (Minimum Temperature)\")\n",
        "plt.ylabel(\"TMAX (Maximum Temperature)\")\n",
        "plt.title(\"TMAX vs. TMIN Correlation\")\n",
        "plt.show()\n",
        "\n",
        "# Calculates and displays the correlation coefficient\n",
        "correlation = df_weather[\"TMIN\"].corr(df_weather[\"TMAX\"])\n",
        "print(f\"Pearson Correlation Coefficient: {correlation:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMrzibalPGKL"
      },
      "source": [
        "Strong positive correlation with an almost perfect linear relationship between TMAX and TMIN which means as TMIN increases, TMAX also increases as expected.\n",
        "\n",
        "TMIN is a strong indicator/predictor of TMAX..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yb5bIahXyS_g"
      },
      "outputs": [],
      "source": [
        "# Summary statistics\n",
        "print(\"ðŸ“Š Precipitation Summary Statistics:\")\n",
        "print(df_weather['PRCP'].describe())\n",
        "\n",
        "# Calculate IQR\n",
        "Q1_prcp = df_weather['PRCP'].quantile(0.25)\n",
        "Q3_prcp = df_weather['PRCP'].quantile(0.75)\n",
        "IQR_prcp = Q3_prcp - Q1_prcp\n",
        "\n",
        "# Define outlier bounds\n",
        "lower_bound_prcp = Q1_prcp - 1.5 * IQR_prcp\n",
        "upper_bound_prcp = Q3_prcp + 1.5 * IQR_prcp\n",
        "\n",
        "# Identify outliers\n",
        "outliers_prcp = (df_weather['PRCP'] < lower_bound_prcp) | (df_weather['PRCP'] > upper_bound_prcp)\n",
        "print(f\"\\nOutlier Summary:\")\n",
        "print(f\"Number of PRCP outliers: {outliers_prcp.sum()}\")\n",
        "print(f\"Upper Bound: {upper_bound_prcp:.3f}, Lower Bound: {lower_bound_prcp:.3f}\")\n",
        "\n",
        "# Plot distribution and boxplot\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "# Histogram\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df_weather['PRCP'], bins=50, kde=True, color='skyblue')\n",
        "plt.title(\"PRCP Distribution (Linear Scale)\")\n",
        "plt.xlabel(\"Precipitation (mm)\")\n",
        "plt.ylabel(\"Count\")\n",
        "\n",
        "# Boxplot\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(x=df_weather['PRCP'], color='lightgray')\n",
        "plt.title(\"Boxplot of PRCP (Linear Scale)\")\n",
        "plt.xlabel(\"Precipitation (mm)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXXkbN084EQV"
      },
      "source": [
        "The first histogram shows that most precipitation values are close to zero, with a long right tail indicating occasional heavier rainfall. The second boxplot reveals seasonal variation, with summer months showing greater variability and more frequent high-precipitation outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vA6em522ay3"
      },
      "outputs": [],
      "source": [
        "# Create a new column for actual PRCP values\n",
        "df_weather['PRCP_actual'] = np.expm1(df_weather['PRCP'])\n",
        "\n",
        "# Filter data to include only non-zero actual precipitation\n",
        "non_zero_prcp = df_weather[df_weather['PRCP_actual'] > 0]\n",
        "\n",
        "# Plot a histogram of non-zero actual precipitation\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.histplot(non_zero_prcp['PRCP_actual'], bins=50, kde=True, color='skyblue')\n",
        "plt.title(\"Distribution of Non-Zero Daily Precipitation\")\n",
        "plt.xlabel(\"Precipitation (mm)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Extract month for analysis\n",
        "df_weather['Month'] = df_weather.index.month\n",
        "\n",
        "# Plot a boxplot of monthly precipitation distribution\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.boxplot(x='Month', y='PRCP_actual', data=df_weather)\n",
        "plt.title(\"Monthly Precipitation Distribution (Boxplot)\")\n",
        "plt.xlabel(\"Month\")\n",
        "plt.ylabel(\"Precipitation (mm)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAl0zk931Jfm"
      },
      "source": [
        "This histogram and boxplot (including zeros) confirm that dry days dominate the dataset, leading to a highly skewed distribution. Overall, precipitation is infrequent but occasionally intense, and its distribution is both sparse and seasonal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVClNDRCZoSn"
      },
      "source": [
        "## Task 1: Predicting Climate (long-term)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we predict future **climate variables**, including **temperature (TMAX and TMIN)** and **precipitation (PRCP)**, using two different machine learning models: **Long Short-Term Memory (LSTM)** and **XGBoost**. Both models are well-suited for time series forecasting, but they approach the problem in different ways:\n",
        "\n",
        "1. **LSTM Model**: The LSTM model is a type of **Recurrent Neural Network (RNN)** that is particularly effective at capturing **long-term dependencies** and **seasonal patterns** in time series data. It will be used to model and forecast **climate trends** over time.\n",
        "   \n",
        "2. **XGBoost Model**: XGBoost is a powerful **gradient boosting algorithm** that performs well on structured/tabular data. In this task, we use XGBoost to predict future climate variables based on **historical climate data**. While not specifically designed for time series forecasting, XGBoost can still capture **trends and relationships** in the data when paired with appropriate feature engineering.\n",
        "\n",
        "The key steps in this task include:\n",
        "1. **Data Preparation**: The climate data will be processed and formatted for both the LSTM and XGBoost models. For LSTM, this involves transforming the data into sequences, while for XGBoost, we'll create **lag features** and other temporal features.\n",
        "2. **Training the Models**: Both the LSTM and XGBoost models will be trained on historical climate data to learn the underlying patterns in **temperature** and **precipitation**.\n",
        "3. **Forecasting**: After training, the models will be used to forecast **future climate values** for TMAX, TMIN, and PRCP over the next **20 years (2025â€“2044)**.\n",
        "4. **Performance Evaluation**: The models will be evaluated using performance metrics such as **Mean Squared Error (MSE)** and **Mean Absolute Error (MAE)** to compare their accuracy and suitability for predicting future climate conditions.\n",
        "\n",
        "By comparing the performance of both models, we aim to understand how well each approach can predict future **climate trends** and assess their potential for **long-term forecasting**."
      ],
      "metadata": {
        "id": "PnKULR_fOp4I"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMWJOWSh6l-7"
      },
      "source": [
        "### Using LTSM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPPbY9Fb4vqN"
      },
      "outputs": [],
      "source": [
        "# Feature columns\n",
        "features = ['TMAX', 'TMIN', 'PRCP', 'Month_sin', 'Month_cos']\n",
        "target = ['TMAX', 'TMIN', 'PRCP']\n",
        "\n",
        "# Adds Month_sin and Month_cos to df_monthly:\n",
        "df_monthly = df_monthly.reset_index() #Reset index to access the 'date' column\n",
        "df_monthly['Month'] = df_monthly['date'].dt.month\n",
        "df_monthly['Month_sin'] = np.sin(2 * np.pi * df_monthly['Month'] / 12)\n",
        "df_monthly['Month_cos'] = np.cos(2 * np.pi * df_monthly['Month'] / 12)\n",
        "\n",
        "# Update features list:\n",
        "features = ['TMAX', 'TMIN', 'PRCP', 'Month_sin', 'Month_cos'] # Include new features\n",
        "\n",
        "# Normalize features\n",
        "scaler = MinMaxScaler()\n",
        "df_scaled = df_monthly.copy()\n",
        "df_scaled[features] = scaler.fit_transform(df_monthly[features])\n",
        "\n",
        "# Prepare sequence data (12-month lookback to predict 13th month)\n",
        "def create_sequences(data, sequence_length=12):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - sequence_length):\n",
        "        X.append(data[i:i+sequence_length])\n",
        "        y.append(data[i+sequence_length][:3])  # Only TMAX, TMIN, PRCP\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "data_array = df_scaled[features].values\n",
        "X, y = create_sequences(data_array)\n",
        "\n",
        "# Split based on date\n",
        "dates = df_monthly['date'].values[12:]\n",
        "split_idx = np.where(pd.to_datetime(dates) < np.datetime64(\"2020-01\"))[0][-1] + 1\n",
        "\n",
        "X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "dates_test = dates[split_idx:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUnM7oc15b6l"
      },
      "outputs": [],
      "source": [
        "# Builds LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=False))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(y_train.shape[1]))  # Output layer matches target size (3)\n",
        "\n",
        "model.compile(optimizer='adam', loss='mae')\n",
        "\n",
        "# Trains Model\n",
        "early_stop = EarlyStopping(patience=5, restore_best_weights=True)\n",
        "model.fit(X_train, y_train, validation_split=0.1, epochs=50, batch_size=16, callbacks=[early_stop], verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sc0XucSy5_VJ"
      },
      "outputs": [],
      "source": [
        "#  Predict on 2021\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Inverse scale for interpretability\n",
        "pad = np.zeros((len(y_pred), 2))\n",
        "y_pred_inv = scaler.inverse_transform(np.hstack([y_pred, pad]))[:, :3]\n",
        "y_test_inv = scaler.inverse_transform(np.hstack([y_test, pad]))[:, :3]\n",
        "\n",
        "# Evaluation\n",
        "mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
        "r2 = r2_score(y_test_inv, y_pred_inv)\n",
        "print(f\"LSTM Climate MAE (2021): {mae:.2f}\")\n",
        "print(f\"LSTM Climate RÂ² Score (2021): {r2:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Fj45zvX5b_q"
      },
      "outputs": [],
      "source": [
        "# Plot Actual vs Predicted with Residuals\n",
        "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(16, 12))\n",
        "variables = ['TMAX', 'TMIN', 'PRCP']\n",
        "colors = ['red', 'green', 'purple']\n",
        "\n",
        "for i in range(3):\n",
        "    # Actual vs Predicted\n",
        "    axes[i, 0].plot(dates_test, y_test_inv[:, i], label=f\"Actual {variables[i]}\", linestyle='--')\n",
        "    axes[i, 0].plot(dates_test, y_pred_inv[:, i], label=f\"Predicted {variables[i]}\", color=colors[i])\n",
        "    axes[i, 0].set_title(f\"2021: Actual vs. Predicted {variables[i]} (LSTM)\")\n",
        "    axes[i, 0].set_xlabel(\"Date\")\n",
        "    axes[i, 0].set_ylabel(variables[i])\n",
        "    axes[i, 0].legend()\n",
        "    axes[i, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Residual plot\n",
        "    residuals = y_test_inv[:, i] - y_pred_inv[:, i]\n",
        "    axes[i, 1].scatter(y_test_inv[:, i], residuals, alpha=0.5)\n",
        "    axes[i, 1].axhline(0, color='red', linestyle='dashed')\n",
        "    axes[i, 1].set_title(f\"2021: Residual Plot for {variables[i]}\")\n",
        "    axes[i, 1].set_xlabel(f\"Actual {variables[i]}\")\n",
        "    axes[i, 1].set_ylabel(\"Residuals\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7Kheddl-7xx"
      },
      "source": [
        "* **Actual vs Predicted TMAX**: The plot compares actual vs predicted TMAX for 2021, showing a reasonable fit but with some discrepancies in certain months.\n",
        "* **Actual vs Predicted TMIN**: Similar to TMAX, TMIN shows a good overall match, but slight differences in the predicted values are visible.\n",
        "* **Actual vs Predicted PRCP**: The predicted precipitation closely follows the actual data, though the model underperforms during peak rainfall months.\n",
        "* **Residuals** : The residual plots for each variable show small variations around 0, indicating that while the model captures general trends, it struggles with outliers, particularly for PRCP.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39Ky1-KM6uZU"
      },
      "source": [
        "### Using XGBoost model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rw2F21y6YzOh"
      },
      "outputs": [],
      "source": [
        "# Convert daily data to weekly & monthly averages\n",
        "df_weekly = df_weather.resample(\"W\").mean()  # Weekly Averages\n",
        "df_monthly = df_weather.resample(\"M\").mean()  # Monthly Averages\n",
        "\n",
        "# Reset index for modeling\n",
        "df_weekly.reset_index(inplace=True)\n",
        "df_monthly.reset_index(inplace=True)\n",
        "\n",
        "# Add cyclical encoding for seasonality\n",
        "df_monthly['Month'] = df_monthly['date'].dt.month\n",
        "df_monthly['Year'] = df_monthly['date'].dt.year\n",
        "\n",
        "# Convert month into cyclic feature (to represent seasonality)\n",
        "df_monthly['Month_sin'] = np.sin(2 * np.pi * df_monthly['Month'] / 12)\n",
        "df_monthly['Month_cos'] = np.cos(2 * np.pi * df_monthly['Month'] / 12)\n",
        "\n",
        "# Lag features (previous years' climate as input)\n",
        "df_monthly['TMAX_lag12'] = df_monthly['TMAX'].shift(12)\n",
        "df_monthly['TMIN_lag12'] = df_monthly['TMIN'].shift(12)\n",
        "df_monthly['PRCP_lag12'] = df_monthly['PRCP'].shift(12)\n",
        "df_monthly['TMAX_lag24'] = df_monthly['TMAX'].shift(24)\n",
        "df_monthly['TMIN_lag24'] = df_monthly['TMIN'].shift(24)\n",
        "df_monthly['PRCP_lag24'] = df_monthly['PRCP'].shift(24)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TC1yPZWVYzSW"
      },
      "outputs": [],
      "source": [
        "# Drop NaN values (from lag features)\n",
        "df_monthly.dropna(inplace=True)\n",
        "\n",
        "# Define input features and target variable\n",
        "features = ['TMAX', 'TMIN', 'PRCP', 'Month_sin', 'Month_cos',\n",
        "            'TMAX_lag12', 'TMIN_lag12', 'PRCP_lag12',\n",
        "            'TMAX_lag24', 'TMIN_lag24', 'PRCP_lag24']\n",
        "\n",
        "target = ['TMAX', 'TMIN', 'PRCP']  # Predict next year's climate\n",
        "\n",
        "# Train on data before 2020, test on 2020 climate data\n",
        "train = df_monthly[df_monthly['Year'] < 2020]\n",
        "test = df_monthly[df_monthly['Year'] == 2020]\n",
        "\n",
        "X_train, y_train = train[features], train[target]\n",
        "X_test, y_test = test[features], test[target]\n",
        "\n",
        "# Train XGBoost model for multi-output regression\n",
        "model_climate = XGBRegressor(objective='reg:squarederror', n_estimators=100)\n",
        "model_climate.fit(X_train, y_train)\n",
        "\n",
        "# Predict the next year's climate\n",
        "y_pred_climate = model_climate.predict(X_test)\n",
        "\n",
        "# Convert predictions to DataFrame\n",
        "y_pred_climate_df = pd.DataFrame(y_pred_climate, columns=target, index=y_test.index)\n",
        "\n",
        "# Compute residuals\n",
        "residuals = y_test - y_pred_climate_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4ttXa9dYzUw"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(14, 12))\n",
        "\n",
        "for i, col in enumerate(target):\n",
        "    # Actual vs Predicted\n",
        "    axes[i, 0].plot(test['date'], y_test[col], label=f\"Actual {col}\", color=\"blue\", linestyle='dashed')\n",
        "    axes[i, 0].plot(test['date'], y_pred_climate_df[col], label=f\"Predicted {col}\", color=\"red\")\n",
        "    axes[i, 0].set_xlabel(\"Year\")\n",
        "    axes[i, 0].set_ylabel(col)\n",
        "    axes[i, 0].legend()\n",
        "    axes[i, 0].set_title(f\"Actual vs. Predicted {col} (Climate Model)\")\n",
        "\n",
        "    # Residuals\n",
        "    axes[i, 1].scatter(y_test[col], residuals[col], alpha=0.5)\n",
        "    axes[i, 1].axhline(0, color='red', linestyle='dashed')\n",
        "    axes[i, 1].set_xlabel(f\"Actual {col}\")\n",
        "    axes[i, 1].set_ylabel(\"Residuals\")\n",
        "    axes[i, 1].set_title(f\"Residual Plot for {col}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Compute errors\n",
        "mae_climate = mean_absolute_error(y_test, y_pred_climate_df)\n",
        "r2_climate = r2_score(y_test, y_pred_climate_df)\n",
        "\n",
        "print(f\"Climate Model MAE: {mae_climate:.2f}\")\n",
        "print(f\"RÂ² Score: {r2_climate:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scy3lJNhAWA9"
      },
      "source": [
        "Adding more lag features increased the MAE slightly. This might be due to increased complexity without meaningful new information. You can experiment by selectively removing the lag features (e.g., keeping only 12-month and 24-month lags) and checking if the MAE improves."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Actual vs Predicted TMAX**: The plot compares actual vs predicted TMAX for 2021, showing a good overall fit, but some smoothing in the predicted values, missing fine variations.\n",
        "* **Actual vs Predicted TMAX**: The TMIN plot similarly matches the overall trend, but the XGBoost model's predictions lack sensitivity to some fluctuations, especially in colder months.\n",
        "* **Actual vs Predicted PRCP**: Precipitation predictions from XGBoost follow the overall trend but miss finer seasonal peaks and valleys.\n",
        "* **Residuals**: The residual plots show minimal errors around 0, with a few outliers indicating occasional mispredictions, particularly for PRCP.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lfsl97IOQptu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparison between XGB and LTSM model"
      ],
      "metadata": {
        "id": "zZHGNzmdRSVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare MAE and RÂ² Score for both models\n",
        "comparison_metrics = pd.DataFrame({\n",
        "    'Model': ['XGBoost', 'LSTM'],\n",
        "    'MAE': [mae_climate, mae_lstm],\n",
        "    'RÂ² Score': [r2_climate, r2_lstm]\n",
        "})\n",
        "\n",
        "comparison_metrics.plot(kind='bar', x='Model', y=['MAE', 'RÂ² Score'], color=['blue', 'green'], legend=True)\n",
        "plt.title('Comparison of XGBoost and LSTM for Climate Prediction')\n",
        "plt.ylabel('Metric Value')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Xuh3Bo5TRZxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "j_eDOOmMRZXM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tWPgNJtZoSn"
      },
      "source": [
        "## Task 2: Predicting Weather (short-term)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this task, we aim to predict daily weather parameters (temperature and precipitation) using machine learning models. The primary focus is to build a model that can accurately forecast weather patterns based on historical data. We will experiment with multiple models, such as **XGBoost** and **LSTM**, and evaluate their performance using common metrics like **Mean Absolute Error (MAE)** and **Root Mean Squared Error (RMSE)**. The ultimate goal is to determine the best model for predicting weather variables, comparing their predictions to actual recorded data and analyzing the forecasted weather over different timeframes.\n",
        "\n",
        "We will specifically focus on predicting **TMAX (maximum temperature)**, **TMIN (minimum temperature)**, and **PRCP (precipitation)** for a given station, comparing the results using both traditional machine learning and deep learning approaches.\n"
      ],
      "metadata": {
        "id": "38fNU7vWSco0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXeJJzoj_vHX"
      },
      "source": [
        "### Using LTSM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3QF4DUeAKJx"
      },
      "outputs": [],
      "source": [
        "# Filter and Scale\n",
        "df_weather_daily = df_weather[[\"TMAX\", \"TMIN\", \"PRCP\"]].copy()\n",
        "df_weather_daily = df_weather_daily[df_weather_daily.index.year <= 2020]\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(df_weather_daily)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Od8z-XfzAUbs"
      },
      "outputs": [],
      "source": [
        "# Create sequences\n",
        "def create_sequences_with_dates(data, dates, seq_length=7):\n",
        "    X, y, date_list = [], [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i+seq_length])\n",
        "        y.append(data[i+seq_length])\n",
        "        date_list.append(dates[i+seq_length])\n",
        "    return np.array(X), np.array(y), np.array(date_list)\n",
        "\n",
        "seq_length = 7\n",
        "X_all, y_all, dates_all = create_sequences_with_dates(scaled_data, df_weather_daily.index, seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D22ygdOBAUeL"
      },
      "outputs": [],
      "source": [
        "# Split into train (pre-2020) and test (2020 only)\n",
        "train_mask = pd.to_datetime(dates_all).year < 2020\n",
        "test_mask = pd.to_datetime(dates_all).year == 2020\n",
        "\n",
        "X_train, y_train = X_all[train_mask], y_all[train_mask]\n",
        "X_test, y_test = X_all[test_mask], y_all[test_mask]\n",
        "dates_test = dates_all[test_mask]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nscY8URjAazz"
      },
      "outputs": [],
      "source": [
        "# Building LSTM Model\n",
        "model = Sequential([\n",
        "    LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(y_train.shape[1])\n",
        "])\n",
        "model.compile(optimizer='adam', loss='mae')\n",
        "early_stop = EarlyStopping(patience=5, restore_best_weights=True)\n",
        "\n",
        "model.fit(X_train, y_train, validation_split=0.1, epochs=50, batch_size=16, callbacks=[early_stop], verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYrDkjfYJguB"
      },
      "outputs": [],
      "source": [
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Inverse scale\n",
        "y_test_inv = scaler.inverse_transform(np.hstack([y_test]))\n",
        "y_pred_inv = scaler.inverse_transform(np.hstack([y_pred]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCBhPIeuJkAa"
      },
      "outputs": [],
      "source": [
        "# Final Plots\n",
        "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(14, 12))\n",
        "labels = ['TMAX', 'TMIN', 'PRCP']\n",
        "\n",
        "for i in range(3):\n",
        "    # Actual vs Predicted\n",
        "    axes[i, 0].plot(dates_test, y_test_inv[:, i], label=f\"Actual {labels[i]}\", linestyle='--')\n",
        "    axes[i, 0].plot(dates_test, y_pred_inv[:, i], label=f\"Predicted {labels[i]}\", color='red')\n",
        "    axes[i, 0].set_title(f\"Actual vs. Predicted {labels[i]} (LSTM, 2020)\")\n",
        "    axes[i, 0].set_xlabel(\"Date\")\n",
        "    axes[i, 0].set_ylabel(labels[i])\n",
        "    axes[i, 0].legend()\n",
        "\n",
        "    # Residuals\n",
        "    residuals = y_test_inv[:, i] - y_pred_inv[:, i]\n",
        "    axes[i, 1].scatter(y_test_inv[:, i], residuals, alpha=0.5)\n",
        "    axes[i, 1].axhline(0, color='red', linestyle='dashed')\n",
        "    axes[i, 1].set_xlabel(f\"Actual {labels[i]}\")\n",
        "    axes[i, 1].set_ylabel(\"Residuals\")\n",
        "    axes[i, 1].set_title(f\"Residual Plot for {labels[i]}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Actual vs Predicted TMAX (LSTM, 2020)**: This plot shows the predicted vs actual TMAX (maximum temperature) for the year 2020. The model successfully captures the overall trends, but discrepancies can be seen in certain months, especially during the sharp temperature peaks and drops.\n",
        "  \n",
        "* **Actual vs Predicted TMIN (LSTM, 2020)**: This plot demonstrates the predicted vs actual TMIN (minimum temperature) for 2020. The LSTM model exhibits a reasonable fit, though the predicted values deviate slightly from actual values during the sharpest changes in temperature.\n",
        "\n",
        "* **Actual vs Predicted PRCP (LSTM, 2020)**: The predicted PRCP (precipitation) is compared with actual data for the year 2020. While the model captures the overall trend, some discrepancies are visible in months with peak precipitation, where the predicted values tend to be underestimated.\n",
        "\n",
        "* **Residual Plots for TMAX, TMIN, and PRCP**: These residual plots show the difference between actual and predicted values. Most residuals are centered around zero, suggesting the model fits the general trends well. However, larger residuals can be observed, particularly during months with extreme values of TMAX, TMIN, or PRCP.\n"
      ],
      "metadata": {
        "id": "g7gtLe2zSokD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FY1UMO0_4VU"
      },
      "source": [
        "### Using XGBoost model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9a7_LBsCOUM"
      },
      "outputs": [],
      "source": [
        "# Convert daily data to weekly & monthly averages\n",
        "df_weather.index = pd.to_datetime(df_weather.index)\n",
        "df_daily = df_weather.copy()\n",
        "\n",
        "# Add cyclical encoding for seasonality\n",
        "df_daily['Month'] = df_daily.index.month\n",
        "df_daily['DayOfWeek'] = df_daily.index.dayofweek\n",
        "df_daily['Month_sin'] = np.sin(2 * np.pi * df_daily['Month'] / 12)\n",
        "df_daily['Month_cos'] = np.cos(2 * np.pi * df_daily['Month'] / 12)\n",
        "\n",
        "# Lag features (past weather conditions)\n",
        "for lag in [1, 3, 7, 14, 30]:\n",
        "    df_daily[f'TMAX_lag{lag}'] = df_daily['TMAX'].shift(lag)\n",
        "    df_daily[f'TMIN_lag{lag}'] = df_daily['TMIN'].shift(lag)\n",
        "    df_daily[f'PRCP_lag{lag}'] = df_daily['PRCP'].shift(lag)\n",
        "\n",
        "# Rolling statistics for trends & variability\n",
        "df_daily['TMAX_rolling_mean7'] = df_daily['TMAX'].rolling(window=7).mean()\n",
        "df_daily['TMIN_rolling_mean7'] = df_daily['TMIN'].rolling(window=7).mean()\n",
        "df_daily['PRCP_rolling_sum7'] = df_daily['PRCP'].rolling(window=7).sum()\n",
        "df_daily['TMAX_rolling_std7'] = df_daily['TMAX'].rolling(window=7).std()\n",
        "df_daily['TMIN_rolling_std7'] = df_daily['TMIN'].rolling(window=7).std()\n",
        "\n",
        "# Day-to-day change (delta feature)\n",
        "df_daily['TMAX_delta'] = df_daily['TMAX'].diff()\n",
        "df_daily['TMIN_delta'] = df_daily['TMIN'].diff()\n",
        "df_daily['PRCP_delta'] = df_daily['PRCP'].diff()\n",
        "\n",
        "# Drop NaN values from rolling stats and lags\n",
        "df_daily.dropna(inplace=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPTZOBsICOXP"
      },
      "outputs": [],
      "source": [
        "# Define input features and target variable\n",
        "features = [\n",
        "    'TMAX', 'TMIN', 'PRCP', 'Month_sin', 'Month_cos', 'DayOfWeek'\n",
        "] + [\n",
        "    f'TMAX_lag{lag}' for lag in [1, 3, 7, 14, 30]\n",
        "] + [\n",
        "    f'TMIN_lag{lag}' for lag in [1, 3, 7, 14, 30]\n",
        "] + [\n",
        "    f'PRCP_lag{lag}' for lag in [1, 3, 7, 14, 30]\n",
        "] + [\n",
        "    'TMAX_rolling_mean7', 'TMIN_rolling_mean7', 'PRCP_rolling_sum7',\n",
        "    'TMAX_rolling_std7', 'TMIN_rolling_std7',\n",
        "    'TMAX_delta', 'TMIN_delta', 'PRCP_delta'\n",
        "]\n",
        "\n",
        "\n",
        "target = ['TMAX', 'TMIN', 'PRCP']  # Predict next day's weather\n",
        "\n",
        "# Train-test split (train on data before 2020, test on 2020)\n",
        "train = df_daily[df_daily.index.year < 2020]\n",
        "test = df_daily[df_daily.index.year == 2020]\n",
        "\n",
        "X_train, y_train = train[features], train[target]\n",
        "X_test, y_test = test[features], test[target]\n",
        "\n",
        "# Train XGBoost model for multi-output regression\n",
        "model_weather = XGBRegressor(objective='reg:squarederror', n_estimators=100)\n",
        "model_weather.fit(X_train, y_train)\n",
        "\n",
        "# Predict the next day's weather\n",
        "y_pred_weather = model_weather.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRTTzR1GGA0B"
      },
      "outputs": [],
      "source": [
        "# Compute errors\n",
        "mae_weather = mean_absolute_error(y_test, y_pred_weather)\n",
        "print(f\"Weather Model MAE: {mae_weather:.2f}\")\n",
        "\n",
        "# Naive baseline (tomorrow = today)\n",
        "naive_pred = test[target].shift(1).dropna()\n",
        "naive_mae = mean_absolute_error(naive_pred, y_test.loc[naive_pred.index])\n",
        "print(f\"Naive Baseline MAE: {naive_mae:.2f}\")\n",
        "\n",
        "# Compute residuals\n",
        "residuals = y_test - pd.DataFrame(y_pred_weather, columns=target, index=y_test.index) # Convert to DataFrame with correct index\n",
        "\n",
        "# Plot actual vs predicted weather\n",
        "plt.figure(figsize=(20, 16))\n",
        "\n",
        "for i, col in enumerate(target):\n",
        "    plt.subplot(3, 2, 2 * i + 1)\n",
        "    plt.plot(test.index, y_test[col], label=f\"Actual {col}\", color=\"blue\", linestyle='dashed')\n",
        "    plt.plot(test.index, y_pred_weather[:, i], label=f\"Predicted {col} (Model)\", color=\"red\")\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(col)\n",
        "    plt.legend()\n",
        "    plt.title(f\"Actual vs. Predicted {col} (Weather Model)\")\n",
        "\n",
        "    # Residual plot\n",
        "    plt.subplot(3, 2, 2 * i + 2)\n",
        "    plt.scatter(y_test[col], residuals[col], color='blue', alpha=0.5)  # Access residuals using column name\n",
        "    plt.axhline(y=0, color='red', linestyle='dashed')\n",
        "    plt.xlabel(f\"Actual {col}\")\n",
        "    plt.ylabel(\"Residuals\")\n",
        "    plt.title(f\"Residual Plot for {col}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MO9_LNuCFUi"
      },
      "source": [
        "* **Actual vs Predicted TMAX (XGBoost, 2020)**: This plot compares actual vs predicted TMAX for 2020 using the XGBoost model. The predictions are almost perfectly aligned with the actual values, showing very little deviation, particularly in the months with temperature extremes. The model performs excellently here.\n",
        "  \n",
        "* **Actual vs Predicted TMIN (XGBoost, 2020)**: Similar to TMAX, the TMIN predictions from XGBoost show excellent accuracy, with minimal discrepancies between actual and predicted values. The model captures the sharp changes in temperature well.\n",
        "\n",
        "* **Actual vs Predicted PRCP (XGBoost, 2020)**: The predicted PRCP (precipitation) data closely follows the actual values for 2020. XGBoost performs very well in tracking changes in rainfall, closely matching the observed values, even during peak rainfall periods.\n",
        "\n",
        "* **Residual Plots for TMAX, TMIN, and PRCP (XGBoost)**: The residuals for XGBoost show almost no deviation from zero, indicating that the model fits the data extremely well. Larger residuals are barely noticeable, reinforcing the high accuracy of the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjJ5rgQvG23A"
      },
      "outputs": [],
      "source": [
        "# Compare performance\n",
        "improvement = ((naive_mae - mae_weather) / naive_mae) * 100\n",
        "print(f\"Model improves over naive baseline by {improvement:.2f}%\")\n",
        "\n",
        "# Plot comparison\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(['Weather Model', 'Naive Model'], [mae_weather, naive_mae], color=['red', 'blue'])\n",
        "plt.ylabel('Mean Absolute Error (MAE)')\n",
        "plt.title('Weather Model vs Naive Baseline')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOCUfsNOJJeR"
      },
      "source": [
        "* **Weather Model (XGB) vs Naive Baseline (MAE Comparison)**: This bar plot compares the Mean Absolute Error (MAE) of the XGB weather model against the naive baseline model. The XGB weather model improves over the naive baseline by 96.21%, indicating that it performs much better than the naive baseline which uses a simple prediction strategy (such as predicting the mean of historical data).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparison between XGB and LTSM model"
      ],
      "metadata": {
        "id": "UK_L8TLLTPYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare MAE of XGBoost and LSTM models\n",
        "model_comparison = pd.DataFrame({\n",
        "    'Model': ['XGBoost', 'LSTM'],\n",
        "    'MAE': [mae_weather, mae_lstm]\n",
        "})\n",
        "\n",
        "model_comparison.plot(kind='bar', x='Model', y='MAE', color=['blue', 'red'], legend=False)\n",
        "plt.title('Model Comparison: XGBoost vs LSTM')\n",
        "plt.ylabel('Mean Absolute Error (MAE)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WbP3GULcTarb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mfPQ44bRTbFE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9v-nuiWZoSo"
      },
      "source": [
        "## Extensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eTvMEGOUlK3"
      },
      "source": [
        "### 1. Can you train a machine learning technique to predict 10 or 20 years into the future?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72yahzF7UlO4"
      },
      "outputs": [],
      "source": [
        "# Load your df_monthly with TMAX, TMIN, PRCP\n",
        "# Add cyclical features\n",
        "df_monthly['Month'] = df_monthly['date'].dt.month\n",
        "df_monthly['Month_sin'] = np.sin(2 * np.pi * df_monthly['Month'] / 12)\n",
        "df_monthly['Month_cos'] = np.cos(2 * np.pi * df_monthly['Month'] / 12)\n",
        "\n",
        "# Feature and target setup\n",
        "features = ['TMAX', 'TMIN', 'PRCP', 'Month_sin', 'Month_cos']\n",
        "target = ['TMAX', 'TMIN', 'PRCP']\n",
        "\n",
        "# Scale features\n",
        "scaler = MinMaxScaler()\n",
        "df_scaled = df_monthly.copy()\n",
        "df_scaled[features] = scaler.fit_transform(df_scaled[features])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWIwgKpHUlST"
      },
      "outputs": [],
      "source": [
        "# Create sequences\n",
        "def create_sequences(data, sequence_length=12):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - sequence_length):\n",
        "        X.append(data[i:i + sequence_length])\n",
        "        y.append(data[i + sequence_length][:3])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "data_array = df_scaled[features].values\n",
        "X, y = create_sequences(data_array)\n",
        "\n",
        "# Train LSTM model\n",
        "model = Sequential([\n",
        "    LSTM(64, input_shape=(X.shape[1], X.shape[2])),\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(y.shape[1])\n",
        "])\n",
        "model.compile(optimizer='adam', loss='mae')\n",
        "model.fit(X, y, epochs=100, batch_size=8, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-wEUjUVUlUt"
      },
      "outputs": [],
      "source": [
        "# Forecasting 20 years ahead\n",
        "forecast_input = data_array[-12:].copy()\n",
        "forecast_months = 20 * 12\n",
        "forecast_preds = []\n",
        "forecast_dates = [df_monthly['date'].iloc[-1] + pd.DateOffset(months=i+1) for i in range(forecast_months)]\n",
        "\n",
        "for i in range(forecast_months):\n",
        "    input_seq = np.expand_dims(forecast_input[-12:], axis=0)\n",
        "    pred = model.predict(input_seq, verbose=0)[0]\n",
        "    forecast_preds.append(pred)\n",
        "\n",
        "    # Generate cyclical month features\n",
        "    date = forecast_dates[i]\n",
        "    month_sin = np.sin(2 * np.pi * date.month / 12)\n",
        "    month_cos = np.cos(2 * np.pi * date.month / 12)\n",
        "    forecast_input = np.vstack([forecast_input, np.array([*pred, month_sin, month_cos])])\n",
        "\n",
        "# Inverse transform predictions\n",
        "padded = np.hstack([forecast_preds, np.zeros((forecast_months, len(features) - 3))])\n",
        "forecast_inv = scaler.inverse_transform(padded)[:, :3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHFUIY2cUlXT"
      },
      "outputs": [],
      "source": [
        "# Plotting final results\n",
        "fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n",
        "labels = ['TMAX (Â°C)', 'TMIN (Â°C)', 'Precipitation (mm)']\n",
        "colors = ['red', 'blue', 'green']\n",
        "\n",
        "for i in range(3):\n",
        "    axes[i].plot(forecast_dates, forecast_inv[:, i], color=colors[i])\n",
        "    axes[i].set_title(f\"20-Year Forecast of {target[i]} (LSTM)\")\n",
        "    axes[i].set_ylabel(labels[i])\n",
        "\n",
        "axes[-1].set_xlabel(\"Year\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMFPFc-RUlS_"
      },
      "source": [
        "### 2. Where will the hottest part of the world be in 20 years time?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this task, we will analyze **predicted temperature changes** for multiple weather stations worldwide between **2025 and 2044**. The stations selected represent a range of geographic locations, including the USA, France, Greece, Jamaica, and more.\n",
        "\n",
        "We will focus on:\n",
        "1. **Predicted TMAX (Temperature Maximum)** changes for each station.\n",
        "2. **Comparing** the temperature changes over the next two decades and visualizing these changes.\n",
        "3. **Ranking stations** in terms of their predicted temperatures for the year 2044.\n",
        "4. **Highlighting** the station with the largest temperature increase.\n",
        "\n",
        "Our goal is to explore which regions are expected to experience the most significant temperature changes and which stations will have the highest average temperatures by 2044.\n"
      ],
      "metadata": {
        "id": "kWPhG2exIcyA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o09xxVPFBnej"
      },
      "outputs": [],
      "source": [
        "# Load Stations and Forecast\n",
        "def build_lstm(input_shape):\n",
        "    \"\"\"Enhanced LSTM model with time-aware architecture\"\"\"\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = LSTM(128, return_sequences=True)(inputs)\n",
        "    x = LSTM(64)(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "\n",
        "    # Time-aware adjustment\n",
        "    time_weights = tf.reshape(tf.linspace(1.0, 1.2, 64), (1, 64))\n",
        "    time_adjusted = x * time_weights\n",
        "\n",
        "    outputs = Dense(1)(time_adjusted)\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(optimizer='adam', loss='mse')  # Added compilation\n",
        "    return model\n",
        "\n",
        "def parse_tmax(ghn, statDict):\n",
        "    \"\"\"Enhanced parsing with temporal features\"\"\"\n",
        "    tmax_data = ghn.getVar(statDict, 'TMAX')\n",
        "    df = pd.DataFrame(tmax_data, columns=['date', 'TMAX'])\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    df.set_index('date', inplace=True)\n",
        "    df = df.resample('YE').mean()  # Yearly resampling\n",
        "    return df\n",
        "\n",
        "def create_sequences(data, seq_length=20):\n",
        "    \"\"\"Create time series sequences\"\"\"\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i+seq_length])\n",
        "        y.append(data[i+seq_length])\n",
        "    return np.array(X), np.array(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "hzebgW9sBnej"
      },
      "outputs": [],
      "source": [
        "# Station data\n",
        "stations = {\n",
        "    \"EUREKA, USA\": \"USW00024213.dly\",\n",
        "    \"POCATELLO, USA\": \"USW00024156.dly\",\n",
        "    \"METHONI, Greece\": \"GR000016734.dly\",\n",
        "    \"GUAM INTL AIRPORT\": \"GQW00041415.dly\",\n",
        "    \"MONTEGO BAY/SANGSTE, Jamaica\": \"JM000078388.dly\",\n",
        "    \"MONT-AIGOUAL, France\": \"FR000007560.dly\",\n",
        "    \"RENNES, France\": \"FR000007130.dly\",\n",
        "    \"TOULOUSE, France\": \"FR000007630.dly\",\n",
        "}\n",
        "\n",
        "forecast_df = pd.DataFrame()\n",
        "trend_results = []\n",
        "\n",
        "for label, file in stations.items():\n",
        "    if not os.path.exists(file):\n",
        "        print(f\"Skipping {label}: File not found\")\n",
        "        continue\n",
        "\n",
        "    # Load and prepare data\n",
        "    statDict = ghn.processFile(file)\n",
        "    df_tmax = parse_tmax(ghn, statDict)\n",
        "\n",
        "    if len(df_tmax) < 20:\n",
        "        print(f\"Skipping {label}: Not enough data\")\n",
        "        continue\n",
        "\n",
        "    # Scale data\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled = scaler.fit_transform(df_tmax[['TMAX']])\n",
        "    X, y = create_sequences(scaled, seq_length=20)\n",
        "\n",
        "    # Build, compile and train model\n",
        "    model = build_lstm((X.shape[1], X.shape[2]))\n",
        "    early_stop = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
        "    model.fit(X, y, epochs=100, batch_size=4, verbose=0, callbacks=[early_stop])\n",
        "\n",
        "    # Generate forecasts\n",
        "    last_seq = scaled[-20:].reshape(1, 20, 1)\n",
        "    future_preds = []\n",
        "    for _ in range(20):\n",
        "        next_val = model.predict(last_seq, verbose=0)[0][0]\n",
        "        future_preds.append(next_val)\n",
        "        last_seq = np.append(last_seq[:, 1:, :], [[[next_val]]], axis=1)\n",
        "\n",
        "    # Inverse transform and store results\n",
        "    future_preds = scaler.inverse_transform(np.array(future_preds).reshape(-1, 1)).flatten()\n",
        "    forecast_df[label] = future_preds\n",
        "\n",
        "# Final processing\n",
        "forecast_df.index = np.arange(2025, 2045)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_BRtk57Bnek"
      },
      "outputs": [],
      "source": [
        "# Display trend analysis\n",
        "print(\"\\nTemperature Trends (2025-2044):\")\n",
        "for station, trend in sorted(trend_results, key=lambda x: x[1], reverse=True):\n",
        "    print(f\"{station}: {trend*10:.2f}Â°C/decade\")\n",
        "\n",
        "# Plot all forecasts with trends\n",
        "plt.figure(figsize=(14, 7))\n",
        "for col in forecast_df.columns:\n",
        "    plt.plot(forecast_df.index, forecast_df[col], label=col)\n",
        "plt.title(\"All Station Forecasts (2025-2044)\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"TMAX (Â°C)\")\n",
        "plt.legend(bbox_to_anchor=(1.05, 1))\n",
        "plt.grid()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This plot shows the predicted **TMAX** (maximum temperature) for multiple weather stations from 2025 to 2044. The x-axis represents the **year**, and the y-axis shows the **predicted temperature in Â°C**.\n",
        "\n",
        "- Each line represents one station, with a different color, providing a clear view of the **temperature trends** over the next two decades.\n",
        "- **Key Observations**:\n",
        "  - **Station with highest increase**: **Eureka, USA** shows a noticeable increase, while **Montego Bay, Jamica** has a relatively small change, suggesting a **more stable climate**.\n",
        "\n",
        "This plot provides a **general overview** of expected warming across various regions, helping to identify places that will warm more than others.\n"
      ],
      "metadata": {
        "id": "-YlkRyXaIT88"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJLRNmYZU9u2"
      },
      "outputs": [],
      "source": [
        "# Plotting final results and analysis\n",
        "\n",
        "# Final year ranking plot\n",
        "final_year = forecast_df.loc[2044]\n",
        "sorted_final = final_year.sort_values(ascending=False)\n",
        "plt.figure(figsize=(10, 5))\n",
        "sorted_final.plot(kind='bar', color='tomato')\n",
        "plt.ylabel(\"TMAX (Â°C)\")\n",
        "plt.title(\"Ranked Station Temperatures in 2044\")\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Hottest place in 2044:\n",
        "hottest_station = sorted_final.idxmax()\n",
        "hottest_value = sorted_final.max()\n",
        "print(f\"\\nðŸ”¥ Hottest in 2044: {hottest_station} with {hottest_value:.2f} Â°C\")\n",
        "\n",
        "# Plot differences from baseline instead of absolute values\n",
        "baseline = forecast_df.loc[2025]\n",
        "relative_df = forecast_df.subtract(baseline, axis=1)\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "for col in relative_df.columns:\n",
        "    plt.plot(relative_df.index, relative_df[col], label=col)\n",
        "plt.title(\"Temperature Change Relative to 2025\")\n",
        "plt.ylabel(\"Î”TMAX (Â°C)\")\n",
        "\n",
        "# Calculate and display the rate of change\n",
        "changes = forecast_df.diff().mean()\n",
        "print(\"Average yearly temperature changes:\")\n",
        "print(changes.sort_values(ascending=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeln3ZIoBnek"
      },
      "source": [
        "### 3. What else can you study with this dataset? Is the sun in Utah a predictor of the rain in Spain?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this task, we explore whether **temperature in Utah** can predict **precipitation in Spain**. To investigate this relationship, we will:\n",
        "\n",
        "1. **Extract Climate Data**: We will extract **daily temperature data (TMAX)** from a station in Utah and **precipitation data (PRCP)** from a station in Spain.\n",
        "2. **Calculate Lagged Correlations**: We will compute the **lagged correlations** between Utahâ€™s temperature and Spainâ€™s precipitation over various time lags. This will help us determine if temperature in Utah has any predictive value for rainfall in Spain with a time delay.\n",
        "3. **Model the Relationship**: Using **cross-correlation functions**, we will analyze if there is a significant correlation between the two variables, and if so, at what time lags.\n",
        "4. **Statistical Testing**: We will perform **Monte Carlo simulations** to test the statistical significance of any observed correlation, ensuring that the relationship is not due to random chance.\n",
        "\n",
        "The goal is to understand if there is any meaningful and statistically significant predictive relationship between **temperature in Utah** and **precipitation in Spain**, as this could have implications for climate modeling and forecasting.\n"
      ],
      "metadata": {
        "id": "8kqitY6_EJVZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q061KpjqBnek"
      },
      "outputs": [],
      "source": [
        "# Function for Monte Carlo simulation\n",
        "def monte_carlo_test(original_corrs, normalized, lags, n_simulations=500):\n",
        "    simulated_peaks = []\n",
        "    for _ in range(n_simulations):\n",
        "        shuffled_prcp = normalized['PRCP'].sample(frac=1, replace=False).reset_index(drop=True)\n",
        "        shuffled_df = pd.DataFrame({\n",
        "            'TMAX': normalized['TMAX'].reset_index(drop=True),\n",
        "            'PRCP': shuffled_prcp\n",
        "        })\n",
        "        corrs = [shuffled_df['TMAX'].shift(lag).corr(shuffled_df['PRCP']) for lag in lags]\n",
        "        max_corr = max([abs(c) for c in corrs if not pd.isna(c)])\n",
        "        simulated_peaks.append(max_corr)\n",
        "\n",
        "    true_peak = max([abs(c) for c in original_corrs if not pd.isna(c)])\n",
        "    p_value = sum(1 for peak in simulated_peaks if peak >= true_peak) / n_simulations\n",
        "\n",
        "    return true_peak, simulated_peaks, p_value\n",
        "\n",
        "# Function for Cross-correlation\n",
        "def cross_correlation(utah_file, prcp_file, variable='TMAX'):\n",
        "    utah_dict = ghn.processFile(utah_file)\n",
        "    prcp_dict = ghn.processFile(prcp_file)\n",
        "\n",
        "    tmaxArray = ghn.getVar(utah_dict, 'TMAX')\n",
        "    prcpArray = ghn.getVar(prcp_dict, 'PRCP')\n",
        "\n",
        "    utah_df = pd.DataFrame(tmaxArray, columns=['date', 'TMAX'])\n",
        "    utah_df['date'] = pd.to_datetime(utah_df['date'])\n",
        "    utah_df = utah_df.set_index('date').resample('M').mean()\n",
        "\n",
        "    prcp_df = pd.DataFrame(prcpArray, columns=['date', 'PRCP'])\n",
        "    prcp_df['date'] = pd.to_datetime(prcp_df['date'])\n",
        "    prcp_df = prcp_df.set_index('date').resample('M').sum()\n",
        "\n",
        "    combined = pd.concat([utah_df, prcp_df], axis=1).dropna()\n",
        "    scaler = StandardScaler()\n",
        "    normalized = pd.DataFrame(scaler.fit_transform(combined), columns=combined.columns, index=combined.index)\n",
        "\n",
        "    lags = np.arange(-12, 13)\n",
        "    correlations = [normalized['TMAX'].shift(lag).corr(normalized['PRCP']) for lag in lags]\n",
        "\n",
        "    # Monte Carlo Simulation\n",
        "    original_corrs = [normalized['TMAX'].shift(lag).corr(normalized['PRCP']) for lag in lags]\n",
        "    true_peak, simulated_peaks, p_value = monte_carlo_test(original_corrs, normalized, lags)\n",
        "\n",
        "    return correlations, lags, true_peak, simulated_peaks, p_value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Kcvz06YBnek"
      },
      "outputs": [],
      "source": [
        "# Utah vs Spain\n",
        "utah_file_spain = \"USW00023183.dly\"\n",
        "spain_file = \"SP000008181.dly\"\n",
        "correlations_spain, lags_spain, true_peak_spain, simulated_peaks_spain, p_value_spain = cross_correlation(utah_file_spain, spain_file)\n",
        "\n",
        "# Utah vs Brazil\n",
        "utah_file_brazil = \"USW00023183.dly\"\n",
        "brazil_file = \"BR001746006.dly\"\n",
        "correlations_brazil, lags_brazil, true_peak_brazil, simulated_peaks_brazil, p_value_brazil = cross_correlation(utah_file_brazil, brazil_file)\n",
        "\n",
        "# Utah vs Philippines\n",
        "utah_file_philippines = \"USW00023183.dly\"\n",
        "philippines_file = \"RP000098444.dly\"\n",
        "correlations_philippines, lags_philippines, true_peak_philippines, simulated_peaks_philippines, p_value_philippines = cross_correlation(utah_file_philippines, philippines_file)\n",
        "\n",
        "# Create Combined Plot\n",
        "fig, ax = plt.subplots(3, 1, figsize=(12, 12), sharex=True)\n",
        "\n",
        "# Plot Utah vs Spain\n",
        "ax[0].plot(lags_spain, correlations_spain, marker='o', label='Utah vs Spain', color='orange')\n",
        "ax[0].axhline(0, color='gray', linestyle='--')\n",
        "ax[0].set_ylabel(\"Correlation Coefficient\")\n",
        "ax[0].set_title(\"Lagged Correlation: Utah Sun vs Spain Rain\")\n",
        "ax[0].grid(True)\n",
        "\n",
        "# Plot Utah vs Brazil\n",
        "ax[1].plot(lags_brazil, correlations_brazil, marker='o', label='Utah vs Brazil', color='blue')\n",
        "ax[1].axhline(0, color='gray', linestyle='--')\n",
        "ax[1].set_ylabel(\"Correlation Coefficient\")\n",
        "ax[1].set_title(\"Lagged Correlation: Utah Sun vs Brazil Rain\")\n",
        "ax[1].grid(True)\n",
        "\n",
        "# Plot Utah vs Philippines\n",
        "ax[2].plot(lags_philippines, correlations_philippines, marker='o', label='Utah vs Philippines', color='green')\n",
        "ax[2].axhline(0, color='gray', linestyle='--')\n",
        "ax[2].set_xlabel(\"Lag (months)\")\n",
        "ax[2].set_ylabel(\"Correlation Coefficient\")\n",
        "ax[2].set_title(\"Lagged Correlation: Utah Sun vs Philippines Rain\")\n",
        "ax[2].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgscjA0rBnek"
      },
      "outputs": [],
      "source": [
        "# Plots Monte Carlo Results for Utah vs Spain\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.hist(simulated_peaks_spain, bins=30, color='lightgray', edgecolor='black', label='Shuffled Peaks')\n",
        "plt.axvline(true_peak_spain, color='red', linestyle='--', label=f'True Peak = {true_peak_spain:.2f}')\n",
        "plt.title(\"Monte Carlo Test: Peak Correlation Significance (Utah vs Spain)\")\n",
        "plt.xlabel(\"Peak Absolute Correlation (|r|)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print p-values\n",
        "print(f\"P-value for Utah vs Spain: {p_value_spain:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot above shows the **lagged correlation** between **Utahâ€™s temperature** (TMAX) and **precipitation** in **Spain**, **Brazil**, and the **Philippines** across a range of time lags (from -12 to +12 months).\n",
        "\n",
        "- **Utah vs Spain (Top plot)**: The correlation between Utahâ€™s temperature and Spainâ€™s rainfall fluctuates in a **sinusoidal pattern**. The peak correlation occurs at a **lag of approximately +3 months**, indicating a slight **delayed effect** between temperature in Utah and rainfall in Spain. This suggests that temperature variations in Utah may influence rainfall patterns in Spain with a **time delay**.\n",
        "  \n",
        "- **Utah vs Brazil (Middle plot)**: The correlation between Utah and Brazil is more **pronounced**, showing a stronger **positive correlation** over a **larger time window** (around 4-6 months), with peaks on either side of the zero lag. This indicates a **more significant relationship** between the temperature in Utah and rainfall in Brazil, suggesting that temperature in Utah might be a stronger predictor for rainfall in Brazil compared to Spain.\n",
        "\n",
        "- **Utah vs Philippines (Bottom plot)**: Similar to the Brazil plot, the correlation between Utahâ€™s temperature and rainfall in the Philippines also shows a **sinusoidal pattern**. However, the peak **positive correlation** is observed around **+5 months**, indicating a **slightly different delay** compared to the other regions, further confirming the **complexity** and **regional differences** in climate interactions.\n",
        "\n",
        "This analysis suggests that there are **delayed effects** where temperature in Utah can influence rainfall patterns in distant regions, but the strength and timing of this effect vary depending on the region.\n"
      ],
      "metadata": {
        "id": "YRnV0YZdD5Z5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md5xXZxsUlY1"
      },
      "source": [
        "### 4. How close do weather stations need to be to provide reliable forecasts at other stations?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdNa9q0kBnel"
      },
      "source": [
        "This task explores how the correlation between monthly climate measurements (TMAX and PRCP) decreases with increasing distance between stations. Using 19 Canadian stations with good data coverage, we compare station pairs, calculate their geographic distance, and analyze the similarity of their time series.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TMAX (Temperature) Correlation vs Distance\n",
        "\n",
        "For TMAX, we calculate the correlation between monthly temperature series from pairs of stations and investigate how this correlation changes as the distance between stations increases. The analysis shows that temperature remains highly spatially correlated even at long distances.\n",
        "\n"
      ],
      "metadata": {
        "id": "7V_e5kMlB598"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I--ZjDqJBnel"
      },
      "outputs": [],
      "source": [
        "# Checking data quality\n",
        "print(\"\\nData Quality Check:\")\n",
        "print(f\"Total stations: {len(forecast_df.columns)}\")\n",
        "print(f\"Time period: {forecast_df.index.min()} to {forecast_df.index.max()}\")\n",
        "print(f\"Missing values: {forecast_df.isna().sum().sum()}\")\n",
        "print(f\"Example station stats:\\n{forecast_df.describe().T.head()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1-y3gUdBnel"
      },
      "outputs": [],
      "source": [
        "# List the chosen station IDs\n",
        "station_ids = [\n",
        "    \"CA002202102\", \"CA002202202\", \"CA002202402\", \"CA002202578\", \"CA002202810\",\n",
        "    \"CA002203058\", \"CA002300501\", \"CA002300904\", \"CA002301000\", \"CA002301102\",\n",
        "    \"CA006092920\", \"CA007034482\", \"CA007045401\", \"CA007047914\", \"CA007054096\",\n",
        "    \"CA007060400\", \"CA007093715\", \"CA007103282\", \"CA007113534\"\n",
        "]\n",
        "\n",
        "# Load metadata using your ghn object\n",
        "station_meta = []\n",
        "for sid in station_ids:\n",
        "    station = ghn.stationDict[sid]\n",
        "    station_meta.append((sid, station.name, station.lat, station.lon))\n",
        "\n",
        "meta_df = pd.DataFrame(station_meta, columns=[\"ID\", \"Name\", \"Lat\", \"Lon\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-6omklGBnel"
      },
      "outputs": [],
      "source": [
        "# Calculate great-circle distance between two lat/lon points using the Haversine formula\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    R = 6371  # Earth's radius in kilometers\n",
        "    dlat = radians(lat2 - lat1)\n",
        "    dlon = radians(lon2 - lon1)\n",
        "    a = sin(dlat / 2)**2 + cos(radians(lat1)) * cos(radians(lat2)) * sin(dlon / 2)**2\n",
        "    return R * 2 * atan2(sqrt(a), sqrt(1 - a))\n",
        "\n",
        "# Extract and aggregate daily data from a .dly file into monthly averages (TMAX) or totals (PRCP)\n",
        "def extract_monthly_series(file_path, variable='TMAX'):\n",
        "    data = []\n",
        "\n",
        "    with open(file_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            # Only extract lines that match the desired variable (e.g., TMAX or PRCP)\n",
        "            if line[17:21] != variable:\n",
        "                continue\n",
        "            year = int(line[11:15])\n",
        "            month = int(line[15:17])\n",
        "            for day in range(1, 32):\n",
        "                val = int(line[21 + (day - 1) * 8:26 + (day - 1) * 8])\n",
        "                if val != -9999:  # Missing data\n",
        "                    try:\n",
        "                        data.append((pd.Timestamp(year=year, month=month, day=day), val / 10.0))  # Convert to Â°C or mm\n",
        "                    except:\n",
        "                        continue  # Skip invalid dates\n",
        "\n",
        "    df = pd.DataFrame(data, columns=[\"date\", variable])\n",
        "    df = df.set_index(\"date\").resample(\"ME\").mean()  # Use sum() for PRCP if needed\n",
        "    return df\n",
        "\n",
        "# Loops over all unique station pairs and compute correlations and distances\n",
        "results = []\n",
        "\n",
        "for id1, id2 in itertools.combinations(station_ids, 2):\n",
        "    # Load monthly time series for each station\n",
        "    ts1 = extract_monthly_series(f\"{id1}.dly\")\n",
        "    ts2 = extract_monthly_series(f\"{id2}.dly\")\n",
        "\n",
        "    # Align by date and drop months without data in both\n",
        "    merged = pd.merge(ts1, ts2, left_index=True, right_index=True, how=\"inner\")\n",
        "    if len(merged) < 24:\n",
        "        continue  # Skip pairs with less than 2 years of overlap\n",
        "\n",
        "    # Compute Pearson correlation between the two series\n",
        "    corr = merged.corr().iloc[0, 1]\n",
        "\n",
        "    # Get lat/lon from metadata and compute distance\n",
        "    lat1, lon1 = meta_df[meta_df[\"ID\"] == id1][[\"Lat\", \"Lon\"]].values[0]\n",
        "    lat2, lon2 = meta_df[meta_df[\"ID\"] == id2][[\"Lat\", \"Lon\"]].values[0]\n",
        "    dist = haversine(lat1, lon1, lat2, lon2)\n",
        "\n",
        "    # Store results\n",
        "    results.append((id1, id2, dist, corr))\n",
        "\n",
        "# Final DataFrame: Station pairs, their distance, and correlation\n",
        "dist_corr_df = pd.DataFrame(results, columns=[\"Station1\", \"Station2\", \"Distance_km\", \"Correlation\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiKIz5kIBnem"
      },
      "outputs": [],
      "source": [
        "def decay(x, a, b, c):  # Exponential decay model\n",
        "    return a * np.exp(-b * x) + c\n",
        "\n",
        "# Clean data: remove rows with NaNs or infs\n",
        "clean_df = dist_corr_df.replace([np.inf, -np.inf], np.nan).dropna(subset=[\"Distance_km\", \"Correlation\"])\n",
        "\n",
        "x = clean_df[\"Distance_km\"].values\n",
        "y = clean_df[\"Correlation\"].values\n",
        "\n",
        "params, _ = curve_fit(decay, x, y, p0=(0.1, 0.0005, 0.9))\n",
        "\n",
        "# Plot with fit\n",
        "x_fit = np.linspace(0, max(x), 500)\n",
        "y_fit = decay(x_fit, *params)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.scatter(x, y, alpha=0.5, label=\"Data\")\n",
        "plt.plot(x_fit, y_fit, color='red', label=\"Exponential Fit\")\n",
        "plt.xlabel(\"Distance Between Stations (km)\")\n",
        "plt.ylabel(\"Correlation (Monthly TMAX)\")\n",
        "plt.title(\"Correlation vs Distance with Exponential Fit\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-IZpdPSBnem"
      },
      "outputs": [],
      "source": [
        "print(f\"Exponential Fit Coefficients:\")\n",
        "print(f\"a (initial boost)     = {params[0]:.4f}\")\n",
        "print(f\"b (decay rate)        = {params[1]:.6f}\")\n",
        "print(f\"c (asymptotic value)  = {params[2]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot shows the correlation between monthly temperature data (TMAX) from pairs of Canadian stations as a function of distance. The high correlations observed at short distances (under 500 km) reflect the **spatial coherence of temperature patterns**. This suggests that nearby stations are highly similar and could be used interchangeably for forecasting purposes.\n",
        "\n",
        "As the distance between stations increases, the correlation decays more slowly but remains relatively high, even at distances over 2000 km. The exponential decay model (shown as the red line) fits the data well, with a **correlation value stabilizing around 0.96** for very distant stations. This indicates that **temperature is a highly spatially autocorrelated variable**, allowing for reliable temperature forecasts across large distances in Canada.\n"
      ],
      "metadata": {
        "id": "LAFzASa5CnvP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtwvPY5BBnem"
      },
      "source": [
        "#### PRCP (Precipitation) Correlation vs Distance\n",
        "\n",
        "In contrast to temperature, precipitation (PRCP) is much more localized. This section explores how the correlation between monthly rainfall series decays faster with distance compared to temperature, reflecting the more variable nature of rainfall patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lH0TMawBnem"
      },
      "outputs": [],
      "source": [
        "# Function to extract monthly PRCP data from a .dly file\n",
        "def extract_monthly_series(file_path, variable='PRCP'):\n",
        "    data = []\n",
        "\n",
        "    # Open the .dly file and read each line\n",
        "    with open(file_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            # Skip lines that don't correspond to the variable we need (PRCP)\n",
        "            if line[17:21] != variable:\n",
        "                continue\n",
        "\n",
        "            # Extract the year and month from the line\n",
        "            year = int(line[11:15])\n",
        "            month = int(line[15:17])\n",
        "\n",
        "            # Loop over the days of the month (1 to 31) to extract daily values\n",
        "            for day in range(1, 32):\n",
        "                val = int(line[21 + (day - 1) * 8:26 + (day - 1) * 8])  # Extract the value for the day\n",
        "                if val != -9999:  # Skip missing values (indicated by -9999)\n",
        "                    try:\n",
        "                        # Convert the value to a timestamp and store it in the data list (dividing by 10 to get the actual value)\n",
        "                        data.append((pd.Timestamp(year=year, month=month, day=day), val / 10.0))\n",
        "                    except:\n",
        "                        continue  # If there's an error in conversion, skip that value\n",
        "\n",
        "    # Create a DataFrame with the data, setting the 'date' column as the index\n",
        "    df = pd.DataFrame(data, columns=[\"date\", variable])\n",
        "    # Resample the data by month (ME = Month End) and sum the precipitation values for each month\n",
        "    return df.set_index(\"date\").resample(\"ME\").sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkjIJlTKBnem"
      },
      "outputs": [],
      "source": [
        "# Initialize an empty list to store results\n",
        "results = []\n",
        "\n",
        "# Loop through all combinations of station pairs (using itertools.combinations to get unique pairs)\n",
        "for id1, id2 in itertools.combinations(station_ids, 2):\n",
        "\n",
        "    # Extract the monthly PRCP data for both stations\n",
        "    ts1 = extract_monthly_series(f\"{id1}.dly\", variable='PRCP')\n",
        "    ts2 = extract_monthly_series(f\"{id2}.dly\", variable='PRCP')\n",
        "\n",
        "    # Merge the two time series data based on the date index, keeping only the common dates\n",
        "    merged = pd.merge(ts1, ts2, left_index=True, right_index=True, how=\"inner\")\n",
        "\n",
        "    # Skip pairs with less than 2 years of overlapping data\n",
        "    if len(merged) < 24:\n",
        "        continue\n",
        "\n",
        "    # Calculate the Pearson correlation coefficient between the two stations' precipitation data\n",
        "    corr = merged.corr().iloc[0, 1]\n",
        "\n",
        "    # Retrieve the latitude and longitude for both stations from the metadata DataFrame\n",
        "    lat1, lon1 = meta_df[meta_df[\"ID\"] == id1][[\"Lat\", \"Lon\"]].values[0]\n",
        "    lat2, lon2 = meta_df[meta_df[\"ID\"] == id2][[\"Lat\", \"Lon\"]].values[0]\n",
        "\n",
        "    # Calculate the geographic distance between the two stations using the Haversine formula\n",
        "    dist = haversine(lat1, lon1, lat2, lon2)\n",
        "\n",
        "    # Append the results (station IDs, distance, and correlation) to the results list\n",
        "    results.append((id1, id2, dist, corr))\n",
        "\n",
        "# Convert the list of results into a DataFrame\n",
        "dist_corr_df = pd.DataFrame(results, columns=[\"Station1\", \"Station2\", \"Distance_km\", \"Correlation\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFmroaMQBnem"
      },
      "outputs": [],
      "source": [
        "# Decay model\n",
        "def decay(x, a, b, c):\n",
        "    return a * np.exp(-b * x) + c\n",
        "\n",
        "# Clean NaNs and infs\n",
        "clean_df = dist_corr_df.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "x = clean_df[\"Distance_km\"].values\n",
        "y = clean_df[\"Correlation\"].values\n",
        "\n",
        "# Fit with reasonable starting guess\n",
        "params, _ = curve_fit(decay, x, y, p0=(0.5, 0.001, 0.05))\n",
        "\n",
        "\n",
        "x_fit = np.linspace(0, max(x), 500)\n",
        "y_fit = decay(x_fit, *params)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.scatter(x, y, alpha=0.6, label=\"Data\")\n",
        "plt.plot(x_fit, y_fit, color='red', label=\"Exponential Fit\")\n",
        "plt.xlabel(\"Distance Between Stations (km)\")\n",
        "plt.ylabel(\"Correlation (Monthly PRCP)\")\n",
        "plt.title(\"PRCP Correlation vs Distance with Exponential Fit\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbjtUGJ6Bnem"
      },
      "outputs": [],
      "source": [
        "print(\"Exponential Fit Parameters:\")\n",
        "print(f\"a (initial boost)     = {params[0]:.4f}\")\n",
        "print(f\"b (decay rate)        = {params[1]:.6f}\")\n",
        "print(f\"c (asymptotic value)  = {params[2]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "he plot shows the correlation between monthly precipitation data (PRCP) from pairs of Canadian stations as a function of distance. Unlike temperature (TMAX), precipitation shows a **much faster decay in correlation** with distance. This indicates that **rainfall patterns are more localized** and driven by smaller-scale weather systems, such as storms, fronts, and geographic features like mountains.\n",
        "\n",
        "At distances of less than 500 km, correlations are already relatively low, and beyond ~1000 km, they drop sharply, stabilizing at **around 0.16**. The exponential decay model (red line) fits this trend well, suggesting that precipitation patterns in Canada are **highly dependent on local factors** and that **station forecasts for rainfall cannot be reliably transferred over large distances**.\n",
        "\n",
        "In summary, while temperature can be predicted with relatively high accuracy over large distances, **precipitation prediction requires much denser station coverage**, as correlations decay significantly with distance.\n"
      ],
      "metadata": {
        "id": "TdBBExwUC4tl"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "fkiGJwl6NItR",
        "H00WS9_gVVuR",
        "AA4NChWz1u0n",
        "kIo6MGASJypZ",
        "1MCW2zoYJoer",
        "wMWJOWSh6l-7",
        "39Ky1-KM6uZU",
        "jXeJJzoj_vHX",
        "0FY1UMO0_4VU",
        "l9v-nuiWZoSo",
        "wMFPFc-RUlS_",
        "qeln3ZIoBnek",
        "Md5xXZxsUlY1",
        "7V_e5kMlB598",
        "TtwvPY5BBnem"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}